import requests
import urllib.parse
import pandas as pd
import os
from config.config import get_db_connection,PUBLISHER_MAPPING
from urllib.parse import urlparse
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def get_publisher_from_url(originallink):
    """ ì›ë³¸ ë§í¬ì—ì„œ ë„ë©”ì¸ì„ ì¶”ì¶œí•˜ê³  ì–¸ë¡ ì‚¬ ì´ë¦„ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ """
    if not originallink:  # ğŸ”´ `originallink`ê°€ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
        return "ì–¸ë¡ ì‚¬"

    parsed_url = urlparse(originallink)
    domain = parsed_url.netloc.replace("www.", "")  # `www.` ì œê±°

    return PUBLISHER_MAPPING.get(domain, "ì§€ë°©ì–¸ë¡ ì‚¬")  # ë§¤ì¹­ ì•ˆ ë˜ë©´ `Unknown` ë°˜í™˜

def fetch_news_from_api(keywords, display=100):
    client_id = os.getenv("NAVER_KEY")
    client_secret = os.getenv("NAVER_SECRET_KEY")

    query = " OR ".join(keywords)
    encoded_query = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}"

    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }

    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        news_items = response.json().get("items", [])
        
        # ğŸ”¹ ì œëª© ë˜ëŠ” ì„¤ëª…ì— ì •í™•í•œ í‚¤ì›Œë“œ í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë‚¨ê¹€
        filtered_news = []
        for news in news_items:
            title = news["title"]
            description = news["description"]
            
            # ëª¨ë“  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ë§Œ ì¶”ê°€ (ë¶€ë¶„ í¬í•¨ëœ ê²ƒ ì œì™¸)
            if all(keyword in title or keyword in description for keyword in keywords):
                filtered_news.append(news)

        print("ğŸ”¹ [ê°•ì œ í•„í„°ë§ëœ ë‰´ìŠ¤]")
        for news in filtered_news:
            print(f"ğŸ“° ì œëª©: {news['title']}")
            print(f"ğŸ“Œ ì„¤ëª…: {news['description']}")
            print(f"ğŸ”— ë§í¬: {news['link']}\n")

        return filtered_news
    else:
        print(f"âŒ Error {response.status_code}: {response.text}")
        return []
    
# DBì— ì¤‘ë³µ í™•ì¸ ë° ë‰´ìŠ¤ ì €ì¥
def save_news_to_db(news_items):
    conn = get_db_connection()
    cursor = conn.cursor()

    for news in news_items:
        title = news["title"]
        publisher = news.get("publisher", "ì–¸ë¡ ì‚¬")  # âœ… ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ê°€

        # ì¤‘ë³µ ê²€ì‚¬: ê°™ì€ ì œëª©ì˜ ê¸°ì‚¬ê°€ DBì— ìˆëŠ”ì§€ í™•ì¸
        cursor.execute("SELECT COUNT(*) FROM news WHERE title = %s", (title,))
        if cursor.fetchone()[0] == 0:
            # ì¤‘ë³µë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ ì €ì¥
            cursor.execute("""
            INSERT INTO news (title, link, description, publisher, category)
            VALUES (%s, %s, %s, %s, %s)
            """, (
                title,
                news["link"],
                news["description"],
                publisher,  # âœ… `publisher` ì €ì¥
                "general"
            ))

    conn.commit()
    conn.close()

def get_random_news_recommendations(keywords):
    conn = get_db_connection()
    
    # ğŸ”¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ê¸°ì‚¬ ê²€ìƒ‰
    keyword_conditions = " OR ".join([f"title LIKE '%%{keyword}%%'" for keyword in keywords])
    query = f"SELECT * FROM news WHERE {keyword_conditions}"
    df = pd.read_sql_query(query, conn)
    conn.close()

    # ğŸ”¹ ê²€ìƒ‰ëœ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ë¹ˆ DataFrame ë°˜í™˜
    if df.empty:
        return pd.DataFrame()

    # ğŸ”¹ TF-IDF ë²¡í„° ë³€í™˜
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(df["title"] + " " + df["description"])

    # ğŸ”¹ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

    # ğŸ”¹ í‰ê· ì ìœ¼ë¡œ ìœ ì‚¬ë„ê°€ ë†’ì€ ê¸°ì‚¬ 9ê°œ ì„ íƒ
    avg_similarity = cosine_sim.mean(axis=0)  # ê° ê¸°ì‚¬ë³„ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
    df["similarity"] = avg_similarity  # ë°ì´í„°í”„ë ˆì„ì— ìœ ì‚¬ë„ ì¶”ê°€
    df = df.sort_values(by="similarity", ascending=False)  # ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬

    # ğŸ”¹ ìƒìœ„ ìœ ì‚¬í•œ ê¸°ì‚¬ ì¤‘ì—ì„œ ëœë¤ìœ¼ë¡œ 9ê°œ ì„ íƒ
    top_articles = df.head(20)  # ìƒìœ„ 20ê°œ ì¤‘ì—ì„œ ëœë¤ ìƒ˜í”Œë§
    return top_articles.sample(n=min(9, len(top_articles)))
