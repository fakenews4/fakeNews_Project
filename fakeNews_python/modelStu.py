import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

# âœ… ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("./dataset/korean_fake_news_train.csv")
valid_df = pd.read_csv("./dataset/korean_fake_news_valid.csv")

# âœ… ë°ì´í„°ì˜ 10%ë§Œ ì‚¬ìš© (í•™ìŠµ ì†ë„ í…ŒìŠ¤íŠ¸ìš©)
train_df = train_df.sample(frac=0.1, random_state=42)
valid_df = valid_df.sample(frac=0.1, random_state=42)


# âœ… ì œëª© + ë³¸ë¬¸ì„ ê²°í•©í•˜ì—¬ í•™ìŠµ
train_df["text"] = train_df["title"] + " " + train_df["content"]
valid_df["text"] = valid_df["title"] + " " + valid_df["content"]

# âœ… Hugging Face Dataset í¬ë§·ìœ¼ë¡œ ë³€í™˜
train_dataset = Dataset.from_pandas(train_df[["text", "label"]])
valid_dataset = Dataset.from_pandas(valid_df[["text", "label"]])

# âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained("monologg/kobert")

# âœ… ë°ì´í„° í† í°í™” í•¨ìˆ˜
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

# âœ… ë°ì´í„°ì…‹ í† í°í™” ì ìš©
train_tokenized = train_dataset.map(preprocess_function, batched=True)
valid_tokenized = valid_dataset.map(preprocess_function, batched=True)

# âœ… ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
dataset = DatasetDict({
    "train": train_tokenized,
    "validation": valid_tokenized
})

# âœ… KoBERT ëª¨ë¸ ë¡œë“œ
model = BertForSequenceClassification.from_pretrained("monologg/kobert", num_labels=2)


# âœ… í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
training_args = TrainingArguments(
    output_dir="./kobert-fake-news",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,  # í•™ìŠµ íšŸìˆ˜ (ì¡°ì • ê°€ëŠ¥)
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=500,
    save_total_limit=2
)

# âœ… Trainer ì •ì˜
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)

# âœ… ëª¨ë¸ í•™ìŠµ ì‹œì‘
trainer.train()

# âœ… í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥
model.save_pretrained("./models/kobert-news-finetuned")
tokenizer.save_pretrained("./models/kobert-news-finetuned")

print("ğŸ‰ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: ./models/kobert-news-finetuned")
