import torch
import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

# âœ… ì¶”ê°€ í•™ìŠµí•  ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ë¡œë“œ (ê°€ì§œ ë‰´ìŠ¤ ì¶”ê°€ëœ ë°ì´í„°ì…‹ ì‚¬ìš©)
new_train_df = pd.read_csv("./dataset/augmented_news_dataset.csv")  # ğŸš¨ ë³€ê²½ëœ ë°ì´í„°ì…‹ ê²½ë¡œ

# âœ… ì œëª© + ë³¸ë¬¸ì„ ê²°í•©í•˜ì—¬ í•™ìŠµ ë°ì´í„° ìƒì„±
new_train_df["text"] = new_train_df["title"] + " " + new_train_df["content"]

# âœ… Hugging Face Dataset í¬ë§·ìœ¼ë¡œ ë³€í™˜
new_train_dataset = Dataset.from_pandas(new_train_df[["text", "label"]])

# âœ… ê¸°ì¡´ì— ì‚¬ìš©í•œ í† í¬ë‚˜ì´ì € ë¡œë“œ (ì´ì „ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ìœ ì§€)
tokenizer = BertTokenizer.from_pretrained("./models/kobert-news-continued")  

# âœ… ë°ì´í„° í† í°í™” í•¨ìˆ˜
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

# âœ… ë°ì´í„°ì…‹ í† í°í™” ì ìš©
new_train_tokenized = new_train_dataset.map(preprocess_function, batched=True)

# âœ… DatasetDict ë³€í™˜ (ê²€ì¦ ë°ì´í„° ì—†ì´ ì¶”ê°€ í•™ìŠµ ê°€ëŠ¥)
new_dataset = DatasetDict({
    "train": new_train_tokenized
})

# âœ… ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ì´ì „ í•™ìŠµëœ ëª¨ë¸ í™œìš©)
model = BertForSequenceClassification.from_pretrained("./models/kobert-news-continued")  

# âœ… GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ í›„ ëª¨ë¸ ì´ë™
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# âœ… ì¶”ê°€ í•™ìŠµìš© íŒŒë¼ë¯¸í„° ì„¤ì •
training_args = TrainingArguments(
    output_dir="./models/kobert-news-v4",  # ğŸš¨ ìƒˆë¡œìš´ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    evaluation_strategy="no",  
    save_strategy="epoch",
    per_device_train_batch_size=8,
    num_train_epochs=2,  # ì ì ˆí•œ epoch ì„¤ì •
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=500,
    save_total_limit=2,
    fp16=True
)

# âœ… Trainer ì •ì˜
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=new_dataset["train"]
)

# âœ… ê¸°ì¡´ ëª¨ë¸ì— ì¶”ê°€ í•™ìŠµ ì§„í–‰
trainer.train()

# âœ… ì¶”ê°€ í•™ìŠµëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("./models/kobert-news-v4")  # ğŸš¨ ìƒˆë¡œìš´ ì¶”ê°€ í•™ìŠµ ëª¨ë¸ ì €ì¥
tokenizer.save_pretrained("./models/kobert-news-v4")

print("ğŸ‰ ê¸°ì¡´ ëª¨ë¸ì— ì¶”ê°€ í•™ìŠµ ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: ./models/kobert-news-v4")
