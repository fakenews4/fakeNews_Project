import torch
import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

# âœ… ì¶”ê°€ í•™ìŠµí•  ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ë¡œë“œ
new_train_df = pd.read_csv("./dataset/fake_real_news_10000_dataset.csv")

# âœ… ì œëª© + ë³¸ë¬¸ì„ ê²°í•©í•˜ì—¬ í•™ìŠµ
new_train_df["text"] = new_train_df["title"] + " " + new_train_df["content"]

# âœ… Hugging Face Dataset í¬ë§·ìœ¼ë¡œ ë³€í™˜
new_train_dataset = Dataset.from_pandas(new_train_df[["text", "label"]])

# âœ… ê¸°ì¡´ì— ì‚¬ìš©í•œ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained("./models/kobert-news-finetuned")

# âœ… ë°ì´í„° í† í°í™” í•¨ìˆ˜
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

# âœ… ë°ì´í„°ì…‹ í† í°í™” ì ìš©
new_train_tokenized = new_train_dataset.map(preprocess_function, batched=True)

# âœ… DatasetDict ë³€í™˜ (ê²€ì¦ ë°ì´í„°ëŠ” ê¸°ì¡´ ëª¨ë¸ì— ë§ì¶° ìƒëµ ê°€ëŠ¥)
new_dataset = DatasetDict({
    "train": new_train_tokenized
})

# âœ… ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ì´ì „ í•™ìŠµëœ ëª¨ë¸ í™œìš©)
model = BertForSequenceClassification.from_pretrained("./models/kobert-news-finetuned")

# âœ… GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ í›„ ëª¨ë¸ ì´ë™
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# âœ… ì¶”ê°€ í•™ìŠµìš© íŒŒë¼ë¯¸í„° ì„¤ì •
training_args = TrainingArguments(
    output_dir="./kobert-fake-news-continued",
    evaluation_strategy="no",  # ì¶”ê°€ í•™ìŠµì´ë¯€ë¡œ ê²€ì¦ ê³¼ì • ìƒëµ ê°€ëŠ¥
    save_strategy="epoch",
    per_device_train_batch_size=8,
    num_train_epochs=2,  # ê¸°ì¡´ ëª¨ë¸ì´ í•™ìŠµëœ ìƒíƒœë¼ì„œ ë„ˆë¬´ ë§ì€ epochëŠ” ë¶ˆí•„ìš”
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=500,
    save_total_limit=2,
    fp16=True
)

# âœ… Trainer ì •ì˜
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=new_dataset["train"]
)

# âœ… ê¸°ì¡´ ëª¨ë¸ì— ì¶”ê°€ í•™ìŠµ ì§„í–‰
trainer.train()

# âœ… ì¶”ê°€ í•™ìŠµëœ ëª¨ë¸ ì €ì¥
model.save_pretrained("./models/kobert-news-continued")
tokenizer.save_pretrained("./models/kobert-news-continued")

print("ğŸ‰ ê¸°ì¡´ ëª¨ë¸ì— ì¶”ê°€ í•™ìŠµ ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: ./models/kobert-news-continued")
